{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7f2b0a09aad7cb80da307b1514edcb24",
     "grade": false,
     "grade_id": "cell-fc9a11bdedebd7ff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Day 5 - Gradient descent for simple linear regression\n",
    "\n",
    "Your goal is to use a simplified version of gradient descent to arrive at good choices for the $\\beta$ coefficients $\\beta_0$ and $\\beta_1$\n",
    "\n",
    "Given a set of $\\beta$ coefficients, we can easily compute an performance measure (such as Residual Sum of Squares) of how well the linear model predicts the data.  Since RSS is a measure of error, we can also think of it as the cost of choosing that particular set of $\\beta$ coefficients, given the data.  If we measured the cost for many values of $\\beta$ selected from a grid, the costs would form a cost surface and, if our grid was sampled at small enough intervals, we could obtain a good estimate for $\\beta$ by selecting the $\\beta$ coefficients for which this cost surface was minimized. \n",
    "\n",
    "However, such a process would be expensive because we would be sampling $s^{(p+1)}$ locations (where $s$ is the number of grid samples per feature, and $p$ is the number of features).  We know that an exponential algorithm such as this is usually not ideal, so perhaps using a guided search which only looks at a tiny fraction of these points would be better.   If we imagine the shape of this cost surface landscape, we can think about being at a specific coordinate ($\\beta$) and our goal is to move in the direction that reduces cost (RSS), then, given some assumptions about the landscape, we could use a fairly efficient search to find the coordinates of minimum cost.  More specifically, if the surface is convex, then we can use an optimization tool like gradient descent to find the minimum point on this cost surface. \n",
    "\n",
    "In this learning activity your goal is to implement a simplified version of gradient descent.  You will do this using the technique you may have learned in the very beginning of a calculus course - finding the instantaneous slope of the cost surface - but in this case we are operating in 2 dimensions, so we must find the gradient in each direction ($\\beta_0$ and $\\beta_1$) by evaluating the function for a small change (epsilon) in each direction.     If the cost function has only one local minimum, then the local minimum is the global minimum and a greedy search such as gradient descent could find it.  You will use this greedy search in two dimensions simultaneously ... the trick is figuring out how to use the gradient to pick the next point ($\\beta_0$,$\\beta_1$) to evaluate. \n",
    "\n",
    "Assuming your simplified gradient descent optimizer is working and reducing the RSS cost by improving the choice of $\\beta$ coefficients, you will still need to determine a stopping criteria... otherwise the process will continue forever, trying to get slightly lower cost from the function.  The instructor has included stopping conditions in a later code cell which calls the function you will code to find the gradients.\n",
    "\n",
    "Caution:  this dataset has very few points, so local gradient estimation using partial gradients in each direction ($\\beta_0$,$\\beta_1$) has very little gradient to descend on.  \n",
    "\n",
    "This will make it slow and highly dependent on your starting beta.  \n",
    "Normally there are other much-better numerical techniques to deal with this...  \n",
    "This exercise is for illustration/learning only... Dont use this \"simplified\" gradient descent code on real problems!\n",
    "\n",
    "Note that the process will likely run several 100Ks of iterations and may take a few moments to converge... so expect your cooling fans to kick in while it is running.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f72b069d15701e894b4bf64ab9487188",
     "grade": false,
     "grade_id": "cell-45897d8501b45585",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "85651dcd12e2b886e65768c44e1c9856",
     "grade": false,
     "grade_id": "cell-48ce7f4b08130bcb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Here is prewritten code to set up the dataset as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c7660b81054b504254033d18dba97291",
     "grade": false,
     "grade_id": "cell-82ba5833d2c4e771",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#code to load your matrix here\n",
    "pretest_scores = np.array([95., 85., 80., 70., 60.]).T  #(5x1 array)\n",
    "y = np.array([[85.,95.,70.,65.,70.]]).T\n",
    "\n",
    "#build the design matrix X\n",
    "X = np.vstack((np.ones(len(pretest_scores)),pretest_scores)).T                         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f52909878035d540d23f8839f58b68bb",
     "grade": false,
     "grade_id": "cell-c844ea7f39b9cdc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Helper Functions and Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "16ee93987456af3aea6024dc703f42dd",
     "grade": false,
     "grade_id": "cell-0f4aa00816919395",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#function to generate line points for plotting\n",
    "def computeline(intercept,slope,start_x,end_x):\n",
    "    points_x=[start_x,end_x]\n",
    "    points_y=[intercept,intercept+slope*end_x]\n",
    "    return points_x, points_y\n",
    "\n",
    "\n",
    "def plot_regression_results(X, y, t, ythat, beta0,beta1):\n",
    "    fig = plt.figure()\n",
    "    plt.axis([0.,100.,0.,100.])\n",
    "    #add the points in black\n",
    "    plt.scatter(X[:,1],y,c='k',marker='x')\n",
    "    #add the predicted final exam score for Aptitude test = 80\n",
    "    plt.scatter(t[0,1],ythat,c='g',marker='o')\n",
    "    #add the student line in blue\n",
    "    points_x,points_y = computeline(beta0,beta1,0,100)\n",
    "    plt.plot(points_x,points_y,c='r')\n",
    "    plt.title('Simple Regression')\n",
    "    plt.ylabel('Final Exam Score')\n",
    "    plt.xlabel('Aptitude test score')\n",
    "    #plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_rss_history(rssHistory):\n",
    "    fig = plt.figure()\n",
    "    #plt.axis([0.,100.,0.,100.])\n",
    "    #add the points in black\n",
    "    plt.plot(rssHistory)\n",
    "    plt.title('Residuals History')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.xlabel('iteration')\n",
    "    #plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20c6f8088f48e21a1c2d938de8d04b87",
     "grade": false,
     "grade_id": "cell-7e1462a6374d6091",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Code for the student's iterative search for the best coefficients\n",
    "Complete the code stub below to compute and return the local gradient of the cost (error) surface in the immediate vicinity of the current values of the coefficients\n",
    "\n",
    "STUDENT CODE - Within the function ```compute_gradient```, use ```numpy's``` ```dot()``` function to perform matrix multiplication (not a loop) to compute $\\hat y$ (```yhat```) for all observations and the corresponding RSS using (```yhat```) the truth values (```y```).  Then, for an epsilon-sized change in each dimension (```beta[0]``` and ```beta[1]```) use the resulting estimates for your datapoints and the resulting epsilon-MSEs to compute a gradient in each direction (```errorGradient0``` for $\\beta_0$'s gradient and ```errorGradient1``` for $\\beta_1$'s gradient).\n",
    "\n",
    "* Challenge 1:  Simply adding epsilon to the beta value would yield a gradient value which was computed slightly off center from the current location of $\\beta$ - which means the algorithm might struggle to converge since it is choosing the next value of $\\beta$ using a slightly incorrect gradient.  Rather than computing costs at beta and beta+epsilon, consider a gradient centered on epsilon (```beta+epsilon```, ```beta-epsilon```)\n",
    "* Challenge 2:  While you could compute these gradients through individual hardcoded equations, you could also perform the calculation as a carefully constructed matrix multiplication... are you up to the challenge?\n",
    "\n",
    "Hint  - determine the desired shape of the output and make the correct matrix multiplication before calling ```dot()```.  You may find it necessary to use ```numpy```'s transpose operator (```.T```) on one or both matrices.\n",
    "\n",
    "Another Hint - If running the algorithm shows that RSS is growing worse, consider the possibility that your gradients signs are flipped... and fix your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ef02713a51fb4e3397fca7dbeaa45dfc",
     "grade": false,
     "grade_id": "cell-6b329fe9a79aff3c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,beta,epsilon):\n",
    "    '''\n",
    "    Given the training data and the current estimate for beta,\n",
    "    compute the gradients in each dimension of beta\n",
    "    Returns gradient\n",
    "    '''\n",
    "    \n",
    "    errorGradient0 = 0   #placeholder - you will write code below to compute this\n",
    "    errorGradient1 = 0   #placeholder - you will write code below to compute this\n",
    "    \n",
    "    #STUDENT CODE HERE \n",
    "    #compute the error gradients (slope of the error surface for a tiny change (epsilon) to the beta values)\n",
    "    # to do this, recompute the value of the linear regression line over all datapoints using a epsilon-different set of coefficents\n",
    "    # hint: If your algorithm appears to be maximizing the cost instead of minimizing the cost, check for sign errors in the gradients\n",
    "    #-----------------------------------------------------------\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    #--------------------------\n",
    "    #END STUDENT CODE\n",
    "    \n",
    "    \n",
    "    return [errorGradient0,errorGradient1]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "682cf23eab18c4c0c65b309f7d52ece8",
     "grade": false,
     "grade_id": "cell-7def39c29c5cb89e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Guess Beta coefficients and evaluate the guess on the test point\n",
    "(Optional) Here you get to define your initial conditions for the coefficients.   You can leave the originals or choose something different to see how the trajectory of the search goes.\n",
    "\n",
    "one starting suggestion is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9365dba84f3233f03c073698d30d614",
     "grade": false,
     "grade_id": "cell-b2dbc39477ab740f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "```beta0 = 50.0``` , ```beta1 = 0.3```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f286a6348bf236c009e676249b4a891",
     "grade": false,
     "grade_id": "cell-89da72ea261f5db3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#beta guess here.  Note that later you will find this using gradient decent\n",
    "#beta = np.array([[26.768,0.644]]).T    #guess the best betas (2 x 1 array)  \n",
    "\n",
    "#STUDENT CODE HERE.... GUESS YOUR BETA VALUES FOR Beta0 and Beta1\n",
    "#suggestion:  make the beta guess bad so you can watch the performance improvements\n",
    "#--------------\n",
    "beta0 = 50.0\n",
    "beta1 = 0.3\n",
    "#------------\n",
    "#END STUDENT CODE\n",
    "\n",
    "\n",
    "beta = np.array([[beta0,beta1]]).T      \n",
    "\n",
    "#print the Betas and X's\n",
    "print('Beta','\\n', beta, '\\n')\n",
    "print('Design Matrix X', '\\n', X, '\\n')\n",
    "\n",
    "#estimate yhat for all datapoints\n",
    "yhat = np.dot(X,beta)\n",
    "\n",
    "print('yhat','\\n',  yhat,'\\n')\n",
    "\n",
    "#find the difference betwen predicted and truth \n",
    "ydiff = yhat-y\n",
    "print('ydif (prediction errors)', '\\n',ydiff, '\\n')\n",
    "\n",
    "#compute RSS\n",
    "rss = np.dot(ydiff.T,ydiff)\n",
    "#compute MSE\n",
    "mse = rss/len(ydiff)\n",
    "#compute RMSE\n",
    "rmse =  np.sqrt(mse)\n",
    "print()\n",
    "print('RSS: ', rss, '\\n')\n",
    "print('MSE: ', mse, '\\n')\n",
    "print('RMSE: ',rmse, '\\n')\n",
    "\n",
    "#make prediction on aptitude test score of 80\n",
    "t = np.array([[1, 80]])\n",
    "ythat = np.dot(t,beta)\n",
    "print('Prediction at Aptitude 80', ythat, '\\n')\n",
    "plot_regression_results(X, y, t, ythat, beta0, beta1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca7bf480971ac9888676fed98235ed4c",
     "grade": false,
     "grade_id": "cell-59bb02e7fc3f9310",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Use gradient descent to find good values for the coefficients\n",
    "\n",
    "Note that the process will likely run several 100Ks of iterations and may take a few moments to converge.  The default stopping iteration is 200K... but if you want to try to get a more presise estimation once your gradient evaluator is working properly, you can increase the ```max_iterations``` value.\n",
    "\n",
    "Caution: If your gradients are pointing in the wrong direction, your beta estimates will force the line AWAY from where it should be and the RSS will grow without bound until it exceeds the max value for the datatype... which usually ends the program.  If this is happening to you, you probably have a sign error in the gradient computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08f26cfb461f8a842e00245ece99bb20",
     "grade": false,
     "grade_id": "cell-dbb0d404f642bdb8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#Iteratively find best betas using gradient descent\n",
    "\n",
    "#caution:  this dataset has very few points, so local gradient estimation has very little\n",
    "#           gradient to go on in some dimensions.  this will make it slow and\n",
    "#           highly dependent on your starting beta.  Dont use this on real problems!\n",
    "#-----------------------------------------------------------\n",
    "\n",
    "\n",
    "#setup the search\n",
    "epsilon = 0.000001  #amount of change to beta; used to find emperical gradients of the cost surface near current beta values\n",
    "convergenceThreshold = 0.00001  #If beta changes less than this amount, finish the search\n",
    "improvement = 99999999.0  #start with an unreasonbly high improvement\n",
    "\n",
    "learning_rate = 10.0\n",
    "iteration_count = 0\n",
    "max_iterations = 200000 #the max iterations you want to allow to prevent runaway processes - \n",
    "# start with a low value for max_iterations until you are sure your gradients are correct\n",
    "done = False\n",
    "\n",
    "rssHistory = []\n",
    "beta0History = []\n",
    "beta1History = []\n",
    "\n",
    "\n",
    "\n",
    "t = np.array([[1, 80]])\n",
    "\n",
    "\n",
    "print(\"Starting beta search\")\n",
    "\n",
    "while not done:\n",
    "    #capture current rss value\n",
    "    iteration_count = iteration_count+1\n",
    "    #print(iterations)\n",
    "    yhatold = np.dot(X,beta)\n",
    "    oldRss = np.dot((yhatold-y).T , (yhatold-y))\n",
    "    \n",
    "    rssHistory.extend(oldRss)\n",
    "    \n",
    "    beta0History.extend([beta[0,0]])\n",
    "    beta1History.extend([beta[1,0]])\n",
    "    \n",
    "    #call the student-written code to find the gradient of the cost surface\n",
    "    #at the current coefficient values (beta)\n",
    "    [rss0Gradient,rss1Gradient] = compute_gradient(X,y,beta,epsilon)\n",
    "    \n",
    "    #compute the updates to the betas using the learning rate\n",
    "    b0Update = (rss0Gradient*learning_rate)\n",
    "    b1Update = (rss1Gradient*learning_rate)\n",
    "\n",
    "    \n",
    "    #compute the new betas\n",
    "    b0new = (beta[0,0]+b0Update).item()\n",
    "    b1new = (beta[1,0]+b1Update).item()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #set the new betas\n",
    "    beta.itemset((0,0),b0new)  \n",
    "    beta.itemset((1,0),b1new)             \n",
    "    \n",
    "    #test for convergence    \n",
    "    #if total change in beta is small then done\n",
    "    done = (iteration_count>max_iterations) | (np.sqrt(b0Update**2+b1Update**2)<convergenceThreshold)\n",
    "    \n",
    "    \n",
    "    #test for amount of improvement in RSS\n",
    "    yhat = np.dot(X,beta)\n",
    "    updatedRss = np.dot((yhat-y).T , (yhat-y))\n",
    "    \n",
    "    \n",
    "    #the following code will print an update after a certain number of iterations\n",
    "    displayEveryIterations=10000  #how many iterations to run before displaying an update\n",
    "    if not np.mod(iteration_count,displayEveryIterations):   \n",
    "        ythat = np.dot(t,beta)\n",
    "        beta0 = beta[0,0]\n",
    "        beta1 = beta[1,0]\n",
    "        plot_regression_results(X, y, t, ythat, beta0,beta1)\n",
    "        improvement = oldRss-updatedRss\n",
    "        print('*** Iteration ',iteration_count,\n",
    "              ' improvement',improvement.item(),  \n",
    "              ' updatedRss: ',updatedRss.item(),\n",
    "              '\\n',\n",
    "              ' B0: ', beta0.item(), ' B1: ', beta1.item(),\n",
    "              '\\n\\n\\n' )\n",
    "\n",
    "       \n",
    "\n",
    "print(\"Done\")\n",
    "print(\"Final Results\")\n",
    "\n",
    "ythat = np.dot(t,beta)\n",
    "beta0 = beta[0,0]\n",
    "beta1 = beta[1,0]\n",
    "plot_regression_results(X, y, t, ythat, beta0,beta1)\n",
    "improvement = oldRss-updatedRss\n",
    "print('*** Iteration ',iteration_count,\n",
    "      ' improvement',improvement.item(),  \n",
    "      ' updatedRss: ',updatedRss.item(),\n",
    "      '\\n',\n",
    "      ' B0: ', beta0.item(), ' B1: ', beta1.item(),\n",
    "      '\\n\\n\\n' )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plot_regression_results(X, y, t, ythat, beta0,beta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b86a8c1f22e13e3538a12b2a08da9ca",
     "grade": false,
     "grade_id": "cell-5fed7cab97a35fce",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#convert list to numpy array\n",
    "\n",
    "#plot_rss_history(rssHistory)\n",
    "\n",
    "rssHistoryArray = np.asarray(rssHistory)\n",
    "beta1HistoryArray = np.asarray(beta1History)\n",
    "beta0HistoryArray = np.asarray(beta0History)\n",
    "\n",
    "fig = plt.figure()\n",
    "#plt.axis([0.,100.,0.,100.])\n",
    "#add the points in black\n",
    "print(rssHistoryArray.shape)\n",
    "plt.plot(rssHistoryArray)\n",
    "plt.title('Residuals History')\n",
    "plt.ylabel('Residual')\n",
    "plt.xlabel('iteration')\n",
    "#plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(beta0History)\n",
    "plt.plot(beta1History)\n",
    "plt.legend([\"beta0\",\"beta1\"])\n",
    "plt.title('Beta History')\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('Beta value')\n",
    "#plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
